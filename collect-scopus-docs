#!/usr/bin/env bash

d=$(date -I -d "3 day ago")
docker exec gup-imports_gup-imports_1 bash -c 'python3 /data/scripts/get-scoups-docs.py -d '"$d"' -o /data/files/scopus-raw -a $SCOPUS_API_KEY' >> /var/tmp/gup-scopus.log 
docker exec gup-imports_gup-imports_1 bash -c 'python3 /data/scripts/index-scopus-docs.py -d /data/files/scopus-raw/'"$d"'' >> /var/tmp/gup-scopus-indexing.log
docker exec gup-imports_gup-imports_1 bash -c 'python3 /data/scripts/transform-scopus-docs.py -s /data/files/scopus-raw/'"$d"' -d /data/files/scopus-normalised/'"$d"'' >> /var/tmp/gup-scopus-normalising.log


# Send the transformed scopus data to gup-admin, index-manager in each
# environment since we only fetch scopus data once due to restrictions
# on the Scopus API usage.
# Typically this procedure would be replaced by sending a message to a
# message broker and having one queue for each receiver.

# LAB
docker exec gup-imports_gup-imports_1 bash -c 'python3 /data/scripts/put-scopus-docs.py -d /data/files/scopus-normalised/'"$d"' -u $INDEX_MANAGER_LAB_URL -a $INDEX_MANAGER_LAB_API_KEY' >> /var/tmp/gup-scopus-put-to-gup-admin-index-manager-lab.log

# STAGING
docker exec gup-imports_gup-imports_1 bash -c 'python3 /data/scripts/put-scopus-docs.py -d /data/files/scopus-normalised/'"$d"' -u $INDEX_MANAGER_STAGING_URL -a $INDEX_MANAGER_STAGING_API_KEY' >> /var/tmp/gup-scopus-put-to-gup-admin-index-manager-staging.log

# PRODUCTION - Omit Production environment since it is not yet in use.
#docker exec gup-imports_gup-imports_1 bash -c 'python3 /data/scripts/put-scopus-docs.py -d /data/files/scopus-normalised/'"$d"' -u $INDEX_MANAGER_URL -a $INDEX_MANAGER_API_KEY' >> /var/tmp/gup-scopus-put-to-gup-admin-index-manager-production.log

